{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlH0lCOttCs5"
   },
   "source": [
    "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUPRHaeetRnT"
   },
   "source": [
    "# Lab 04: Experiment Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bry3Hr-PcgDs"
   },
   "source": [
    "### What You Will Learn\n",
    "\n",
    "- Why experiment management is so important for ML model development\n",
    "- Which features of experiment management we use in developing the Text Recognizer\n",
    "- How to use Weights & Biases for experiment management, including metric logging, artifact versioning, and hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs0LXXlCU6Ix"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkQiK7lkgeXm"
   },
   "source": [
    "If you're running this notebook on Google Colab,\n",
    "the cell below will run full environment setup.\n",
    "\n",
    "It should take about three minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVx7C7H0PIZC"
   },
   "outputs": [],
   "source": [
    "lab_idx = 4\n",
    "\n",
    "if \"bootstrap\" not in locals() or bootstrap.run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
    "    import bootstrap\n",
    "\n",
    "    # change into the lab directory\n",
    "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
    "\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    # needed for inline plots in some contexts\n",
    "    %matplotlib inline\n",
    "\n",
    "    bootstrap.run = False  # change to True re-run setup\n",
    "    \n",
    "!pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab contains a large number of embedded IFrames.\n",
    "\n",
    "This cell makes the notebook wider if you set `full_width` to `True`.\n",
    "\n",
    "Particularly useful in local Jupyter. Colab defaults to full width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, IFrame\n",
    "\n",
    "full_width = True\n",
    "frame_height = 720  # adjust for your screen\n",
    "\n",
    "if full_width:  # if we want the notebook to take up the whole width\n",
    "    # add styling to the notebook's HTML directly\n",
    "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "    display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPoFCoEcC8SV"
   },
   "source": [
    "# Why experiment management?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run an experiment.\n",
    "\n",
    "We'll train a new model on a new dataset.\n",
    "\n",
    "> <small> Simplified model since [Lab 03](https://fsdl.me/lab03-colab) --\n",
    "> still a CNN encoder with a Transformer decoder,\n",
    "> still on real text, but only one line at a time.\n",
    "> Much easier, quicker to run, but still fairly realistic.\n",
    "> Line-level recognition is [common](https://huggingface.co/docs/transformers/model_doc/trocr). </small>\n",
    "\n",
    "It will take up to a few minutes.\n",
    "\n",
    "As it's running, continue reading below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "\n",
    "gpus = int(torch.cuda.is_available()) \n",
    "\n",
    "%run training/run_experiment.py --model_class LineCNNTransformer --data_class IAMLines \\\n",
    "  --loss transformer --batch_size 32 --gpus {gpus} --max_epochs 2 \\\n",
    "  --limit_train_batches 0.1 --limit_val_batches 0.1 --limit_test_batches 0.1 --log_every_n_steps 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're calculating lots of metrics and reporting them to the terminal here.\n",
    "\n",
    "Achieved by the built-in `.log` method of the `LightningModule`, very straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is lost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- metric values except most recent\n",
    "- timestamps\n",
    "- stdout\n",
    "- CLI args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model weights outside the top 5\n",
    "- system info\n",
    "- git information / disk state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is like compilation:\n",
    "in the \"differentiable software\" sense\n",
    "and in that there's lots of arcane flags.\n",
    "\n",
    "Even more so than normal compilation,\n",
    "generates a ton of important data for understanding future behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could save this information ourselves but\n",
    "- bunch of boilerplate code\n",
    "- conflict over resources\n",
    "- how do we view the information once it is saved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning to read information\n",
    "[from streaming numbers in the command line](http://www.quickmeme.com/img/45/4502c7603faf94c0e431761368e9573df164fad15f1bbc27fc03ad493f010dea.jpg)\n",
    "is something of a rite of passage for MLEs,\n",
    "there's a better way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Experiment Tracking with Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this information end up in the output?\n",
    "\n",
    "Review the `training_step` method of our `LightningModule` class,\n",
    "the `TransformerLitModel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.lit_models.transformer import TransformerLitModel\n",
    "\n",
    "\n",
    "TransformerLitModel.validation_step??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on `validation/loss` and `validation/cer` for now.\n",
    "\n",
    "The `self.log` method with `prog_bar=True` puts information about metrics in the progress bar.\n",
    "Read more about `log`\n",
    "[here](https://pytorch-lightning.readthedocs.io/en/1.6.1/common/lightning_module.html#train-epoch-level-metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the `self.log` method of the `LightningModule` isn't just for logging to the terminal --\n",
    "it can also use a logger to push information elsewhere.\n",
    "\n",
    "By default, we use\n",
    "[TensorBoard](https://www.tensorflow.org/tensorboard)\n",
    "via the Lightning `TensorBoardLogger`,\n",
    "which saves results to the local disk.\n",
    "\n",
    "Let's find them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a sequence of bash commands to get the latest checkpoint's filename\n",
    "#  by hand, you can just copy and paste it\n",
    "\n",
    "list_all_log_files = \"find training/logs/lightning_logs/\"  # find avoids issues with \\n in filenames\n",
    "filter_to_folders = \"grep '_[0-9]*$'\"  # regex match on end of line\n",
    "sort_version_descending = \"sort -Vr\"  # uses \"version\" sorting (-V) and reverses (-r)\n",
    "take_first = \"head -n 1\"  # the first n elements, n=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_log, = ! {list_all_log_files} | {filter_to_folders} | {sort_version_descending} | {take_first}\n",
    "latest_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -lh {latest_log}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view results, we need to launch a TensorBoard server --\n",
    "much like we need to launch a Jupyter server to use Jupyter notebooks.\n",
    "\n",
    "The cell below loads an extension that lets you use TensorBoard inside of a notebook\n",
    "the same way you'd use it from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# same command works in terminal, with \"{arguments}\" replaced with values or \"$VARIABLES\"\n",
    "\n",
    "port = 11717  # pick an open port on your machine\n",
    "host = \"0.0.0.0\" # allow connections from the internet\n",
    "                 #   watch out! make sure you turn TensorBoard off\n",
    "\n",
    "%tensorboard --logdir {latest_log} --port {port} --host {host}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've run many experiments on this machine,\n",
    "you can see all of their results by pointing TensorBoard\n",
    "at the whole `lightning_logs` directory,\n",
    "rather than just one experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir training/logs/lightning_logs --port {port + 1} --host \"0.0.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large numbers of experiments, the management experience is not great,\n",
    "and it's especially difficult to move across groups of experiments or to collaborate,\n",
    "which are important as applications mature and teams grow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard is an independent service, so we need to make sure we turn it off when we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard.notebook\n",
    "\n",
    "# get the process IDs for all tensorboard instances\n",
    "pids = [tb.pid for tb in tensorboard.notebook.manager.get_all()]\n",
    "\n",
    "done_with_tensorboard = False\n",
    "\n",
    "if done_with_tensorboard:\n",
    "    for pid in pids:\n",
    "        !kill -9 {pid} 2> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Management with Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What tools are available for experiment management?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is powerful and flexible and very scalable,\n",
    "but running it requires engineering effort and babysitting --\n",
    "you're running a database, writing data to it,\n",
    "and layering a web application over it it.\n",
    "This is a fairly common workflow for web developers,\n",
    "but not so much for ML engineers.\n",
    "\n",
    "Can use [tensorboard.dev](https://tensorboard.dev/),\n",
    "and it's as simple as running the command `tensorboard dev upload`\n",
    "pointed at your logging directory.\n",
    "\n",
    "But there are strict limits to the free tier:\n",
    "1GB of tensor data and 1GB of binary data.\n",
    "A single Text Recognizer compiled model checkpoint is ~500MB,\n",
    "and that's not particularly large for a useful model.\n",
    "Furthermore, all data is public --\n",
    "tensorboard.dev works very well for academic and open projects\n",
    "but not for industrial ML.\n",
    "\n",
    "Alternatively,\n",
    "could use [git LFS](https://git-lfs.github.com/)\n",
    "to track binary data and tensor data,\n",
    "which is more likely to be sensitive than metrics.\n",
    "But this separation is un-natural:\n",
    "model binaries, metrics,\n",
    "and tracked inputs/outputs\n",
    "are all needed to debug model development pipelines,\n",
    "and fragmenting them across services makes debugging harder.\n",
    "Additionally, git-style versioning is an awkward fit for logging --\n",
    "is it really sensible to create a new commit for each logging event?\n",
    "\n",
    "The Hugging Face ecosystem uses TensorBoard and git LFS.\n",
    "The Hugging Face Hub, a git server much like GitHub,\n",
    "[will host TensorBoard alongside models](https://huggingface.co/docs/hub/tensorboard)\n",
    "and officially has\n",
    "[no storage limit](https://discuss.huggingface.co/t/is-there-a-size-limit-for-dataset-hosting/14861/4),\n",
    "avoiding the\n",
    "[tight bandwidth and storage limits](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-storage-and-bandwidth-usage),\n",
    "that make using git LFS with GitHub infeasible.\n",
    "Using the hub requires maintaining an additional git remote or switching from GitHub,\n",
    "which is infeasible for many projects.\n",
    "\n",
    "There are multiple alternatives to TensorBoard.\n",
    "The primary [open governance](https://www.ibm.com/blogs/cloud-computing/2016/10/27/open-source-open-governance/)\n",
    "tool is [MLflow](https://github.com/mlflow/mlflow/)\n",
    "and there are a number of\n",
    "[closed-governance and/or closed-source tools](https://www.reddit.com/r/MachineLearning/comments/q5g7m9/n_sagemaker_experiments_vs_comet_neptune_wandb_etc/).\n",
    "\n",
    "These tools generally avoid any need to worry about hosting\n",
    "(unless data governance rules require a self-hosted version).\n",
    "\n",
    "Among them, the FSDL recommendation is\n",
    "[Weights & Biases](https://wandb.ai),\n",
    "which we believe offers the best user experience,\n",
    "the best integrations with other tools,\n",
    "including\n",
    "[Lightning](https://docs.wandb.ai/guides/integrations/lightning) and\n",
    "[Keras](https://docs.wandb.ai/guides/integrations/keras),\n",
    "[Jupyter](https://docs.wandb.ai/guides/track/jupyter),\n",
    "and even\n",
    "[TensorBoard](https://docs.wandb.ai/guides/integrations/tensorboard),\n",
    "and the best tools for collaboration.\n",
    "For a broad set of opinions on experiment management tools,\n",
    "see these discussions:\n",
    "\n",
    "- r/mlops: [1](https://www.reddit.com/r/mlops/comments/uxieq3/is_weights_and_biases_worth_the_money/), [2](https://www.reddit.com/r/mlops/comments/sbtkxz/best_mlops_platform_for_2022/)\n",
    "- r/MachineLearning: [3](https://www.reddit.com/r/MachineLearning/comments/sqa36p/comment/hwls9px/?utm_source=share&utm_medium=web2x&context=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "print(wandb.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integration is simple:\n",
    "we get most of it just by changing a single variable, `logger`, from\n",
    "`TensorboardLogger` to `WandbLogger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"args.wandb\" -A 5 training/run_experiment.py | head -n 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to complete the rest of this notebook,\n",
    "you'll need a Weights & Biases account.\n",
    "\n",
    "As with GitHub, the free tier is very generous for work that is open to the public\n",
    "but much more limited for work that is private.\n",
    "\n",
    "The Text Recognizer project will fit comfortably within the free tier.\n",
    "\n",
    "Run the cell below and follow the prompts to log in or create an account or go\n",
    "[here](https://wandb.ai/signup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to launch an experiment tracked with Weights & Biases.\n",
    "\n",
    "The experiment can take between 3 and 10 minutes to run.\n",
    "In that time, continue reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%run training/run_experiment.py --model_class LineCNNTransformer --data_class IAMLines \\\n",
    "  --loss transformer --batch_size 32 --gpus {gpus} --max_epochs 10 \\\n",
    "  --log_every_n_steps 10 --wandb --limit_test_batches 0.1 \\\n",
    "  --limit_train_batches 0.1 --limit_val_batches 0.1\n",
    "    \n",
    "last_expt = wandb.run\n",
    "\n",
    "wandb.finish()  # necessary in this style of notebook execution, not necessary in CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "additional info from wandb\n",
    "- data saved locally\n",
    "- data also synced to their servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's `wandb` doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "launches a separate process to \"listen\" for events and upload them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at the end:\n",
    "- sparklines\n",
    "- summary metrics\n",
    "\n",
    "These also show up in the command line,\n",
    "with a little less pizzazz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interface for W&B is a web application.\n",
    "\n",
    "To view results, head to the link to the notebook output\n",
    "as \"Syncing run **{adjective}-{noun}-{number}**\".\n",
    "\n",
    "You can watch results stream in live at that link.\n",
    "\n",
    "Once training is finished, you can run the cell below below to print the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(last_expt.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For even more convenience, we can also see the results directly in the notebook by embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(last_expt.url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the [run page](https://docs.wandb.ai/ref/app/pages/run-page).\n",
    "\n",
    "What do we have here?\n",
    "\n",
    "A bunch of tabs with information about our experiment.\n",
    "\n",
    "From top to bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Overview](https://docs.wandb.ai/ref/app/pages/run-page#overview-tab)\n",
    "  - (i) icon\n",
    "  - high-level info\n",
    "  - git repo and state\n",
    "  - system hardware and hostname\n",
    "- Charts\n",
    "  - line plot icon\n",
    "  - come back to this\n",
    "- [System](https://docs.wandb.ai/ref/app/pages/run-page#system-tab)\n",
    "  - computer chip icon\n",
    "  - GPU metrics, CPU metrics, I/O and network metrics\n",
    "- [Logs](https://docs.wandb.ai/ref/app/pages/run-page#logs-tab)\n",
    "  - command prompt icon\n",
    "  - stdout\n",
    "- Model\n",
    "  - undirected graph icon, not super helpful\n",
    "- [Files](https://docs.wandb.ai/ref/app/pages/run-page#files-tab)\n",
    "  - documents icon\n",
    "  - conda-environment.yaml and requirements.txt\n",
    "  - diff.patch\n",
    "- [Artifacts](https://docs.wandb.ai/ref/app/pages/run-page#artifacts-tab)\n",
    "  - drum storage icon, aka \"stacked hockey pucks\"\n",
    "  - versioned binary files: `run_table`s of predictions and `model` checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Charts redux\n",
    "  - you can edit the charts\n",
    "  - peep the gradient histos -- `wandb.watch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have model inputs and outputs.\n",
    "This requires a bit more work than everything else,\n",
    "which is part of the W&B features built into Lightning.\n",
    "\n",
    "This is achieved by a custom Lightning `Callback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.callbacks.imtotext import ImageToTextTableLogger\n",
    "\n",
    "\n",
    "ImageToTextTableLogger??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It logs structured tabular data to W&B,\n",
    "where the columns can be rich media\n",
    "and the tables support basic exploratory data analysis in the browser.\n",
    "[Docs here](https://docs.wandb.ai/guides/data-vis/log-tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_versions_url = last_expt.url.split(\"runs\")[0] + f\"artifacts/run_table/run-{last_expt.id}-trainpredictions/\"\n",
    "table_data_url = table_versions_url + \"v0/files/train/predictions.table.json\"\n",
    "\n",
    "print(table_data_url)\n",
    "IFrame(src=table_data_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also view lots of runs at once: [project page](https://docs.wandb.ai/ref/app/pages/project-page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what this looks like for a longer-running project --\n",
    "some of the debugging and feature addition work updating the course from 2021 to 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "project_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/workspace\"\n",
    "\n",
    "print(project_url)\n",
    "IFrame(src=project_url, width=\"100%\", height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store and version large binary files in the W&B cloud. [Docs](https://docs.wandb.ai/guides/artifacts/artifacts-core-concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on one of the `model` checkpoints -- whichever version.\n",
    "\n",
    "Overview: includes which run created this model checkpoint.\n",
    "\n",
    "Metadata: includes hyperparameters and `validation/cer`.\n",
    "\n",
    "Files: actual file contents of the artifact. Different between versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IFrame(src=last_expt.url + \"/artifacts\", width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storage limits,\n",
    "as of August 2022:\n",
    "  - 100GB of Artifacts\n",
    "  - 100GB of experiment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can track your storage and compare it to limits at this URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_tracker_url = f\"https://wandb.ai/usage/{last_expt.entity}\"\n",
    "\n",
    "print(storage_tracker_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also programmatically access data via an API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_api = wandb.Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can access the data we just logged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wb_api.run(\"/\".join([last_expt.entity, last_expt.project, last_expt.id]))  # fetch a run given your username, the project, and the run's ID\n",
    "\n",
    "hist = run.history()  # and pull down a sample of the data as a pandas DataFrame\n",
    "\n",
    "hist.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.groupby(\"epoch\")[\"train/loss\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "including the artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which artifacts where created and logged?\n",
    "artifacts = run.logged_artifacts()\n",
    "\n",
    "for artifact in artifacts:\n",
    "    print(f\"artifact of type {artifact.type}: {artifact.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meaning we can easily recreate training or validation data that came out of our `DataLoader`s,\n",
    "which is normally ephemeral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "artifact = wb_api.artifact(f\"{last_expt.entity}/{last_expt.project}/run-{last_expt.id}-trainpredictions:latest\")\n",
    "artifact_dir = Path(artifact.download(root=\"training/logs\"))\n",
    "image_dir = artifact_dir / \"media\" / \"images\"\n",
    "\n",
    "images = [path for path in image_dir.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(str(random.choice(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced W&B API Usage: MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of a well-instrumented experiment tracking system is that it allows\n",
    "automatic relation of information:\n",
    "what were the inputs when this model's gradient spiked?\n",
    "which models have been trained on this dataset,\n",
    "and what was their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below pull down the training data\n",
    "for the model currently running the FSDL Text Recognizer app.\n",
    "\n",
    "This is just intended as a demonstration of what's possible,\n",
    "so don't worry about understanding every piece of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from the same project that we used to look at the project view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_recognizer_project = wb_api.project(\"fsdl-text-recognizer-2021-training\", entity=\"cfrye59\")\n",
    "\n",
    "text_recognizer_project  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we search it for the text recognizer model currently being used in production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all versions of the text-recognizer ever put into production by...\n",
    "\n",
    "for art_type in text_recognizer_project.artifacts_types(): # looking through all artifact types\n",
    "    if art_type.name == \"prod-ready\":  # for the prod-ready type\n",
    "        # and grabbing the text-recognizer\n",
    "        production_text_recognizers = art_type.collection(\"paragraph-text-recognizer\").versions()\n",
    "\n",
    "# and then get the one that's currently being tested in CI by...\n",
    "for text_recognizer in production_text_recognizers:\n",
    "    if \"ci-test\" in text_recognizer.aliases:  # looking for the one that's labeled as CI-tested\n",
    "        in_prod_text_recognizer = text_recognizer\n",
    "\n",
    "# view its metadata at the url or in the notebook\n",
    "in_prod_text_recognizer_url = text_recognizer_project.url[:-9] + f\"artifacts/{in_prod_text_recognizer.type}/{in_prod_text_recognizer.name.replace(':', '/')}\"\n",
    "\n",
    "print(in_prod_text_recognizer_url)\n",
    "IFrame(src=in_prod_text_recognizer_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From its metadata, we can get information about how it was \"staged\" to be put into production,\n",
    "and in particular which model checkpoint was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_run = in_prod_text_recognizer.logged_by()\n",
    "\n",
    "training_ckpt, = [at for at in staging_run.used_artifacts() if at.type == \"model\"]\n",
    "training_ckpt.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That checkpoint was logged by a training experiment, which is available as metadata.\n",
    "\n",
    "We can look at the training run for that model, either here in the notebook or at its URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_run = training_ckpt.logged_by()\n",
    "print(training_run.url)\n",
    "IFrame(src=training_run.url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and from there, we can pull down the logged data and analyze it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = training_run.history(samples=10000)\n",
    "training_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = training_results.groupby(\"epoch\")[\"train/loss\"].mean().plot();\n",
    "training_results[\"validation/loss\"].dropna().plot(logy=True); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data deluge:\n",
    "if you're spun up on the project,\n",
    "it's useful for exploration, discovery.\n",
    "\n",
    "If not, not so useful -- just overwhelming.\n",
    "\n",
    "We need to synthesize the raw logged data into information.\n",
    "This helps us communicate with other stakeholders,\n",
    "preserve knowledge and prevent repetition of work,\n",
    "and surface insights faster.\n",
    "\n",
    "These workflows are supported by the W&B Reports feature\n",
    "([docs here](https://docs.wandb.ai/guides/reports)).\n",
    "\n",
    "Below are some common use cases and an example for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured subset of output from experiment,\n",
    "designed for quickly surfacing issues or insights\n",
    "\n",
    "Use cases:\n",
    "- basic state of ongoing experiment\n",
    "- comparing one experiment to another\n",
    "- spinning yourself back up into context more quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/reports/Training-Run-2022-06-02--VmlldzoyMTAyOTkw\"\n",
    "\n",
    "IFrame(src=dashboard_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One or a small number of charts making a clear point and connected to VCS state.\n",
    "\n",
    "Use cases:\n",
    "- intra-team communication\n",
    "- record-keeping that points to raw info and makes it discoverable\n",
    "- improving confidence in PR correctness (see troubleshooting & testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugfix_doc_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/reports/Overfit-Check-After-Refactor--VmlldzoyMDY5MjI1\"\n",
    "\n",
    "IFrame(src=bugfix_doc_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Blog Post\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of prose, outlinks, context.\n",
    "\n",
    "- external comms: branding, recruiting\n",
    "- communication between teams\n",
    "\n",
    "Example, from the Craiyon.ai project DALL·E Mini project, by FSDL alumnus\n",
    "[Boris Dayma](https://twitter.com/borisdayma)\n",
    "and others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src=\"https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA#training-dall-e-mini\", width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples from the FSDL Text Recognizer project\n",
    "[here](https://wandb.ai/cfrye59/fsdl-text-recognizer-2021-training/reports/-Report-of-Reports---VmlldzoyMjEwNDM5)\n",
    "-- all of them organized into a Report!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also programmatically access data via an API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_api = wandb.Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can access the data we just logged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wb_api.run(\"/\".join([last_expt.entity, last_expt.project, last_expt.id]))  # fetch a run given your username, the project, and the run's ID\n",
    "\n",
    "hist = run.history()  # and pull down a sample of the data as a pandas DataFrame\n",
    "\n",
    "hist.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.groupby(\"epoch\")[\"train/loss\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "including the artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which artifacts where created and logged?\n",
    "artifacts = run.logged_artifacts()\n",
    "\n",
    "for artifact in artifacts:\n",
    "    print(f\"artifact of type {artifact.type}: {artifact.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meaning we can easily recreate training or validation data that came out of our `DataLoader`s,\n",
    "which is normally ephemeral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "artifact = wb_api.artifact(f\"{last_expt.entity}/{last_expt.project}/run-{last_expt.id}-trainpredictions:latest\")\n",
    "artifact_dir = Path(artifact.download(root=\"training/logs\"))\n",
    "image_dir = artifact_dir / \"media\" / \"images\"\n",
    "\n",
    "images = [path for path in image_dir.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image(str(random.choice(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced W&B API Usage: MLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of a well-instrumented experiment tracking system is that it allows\n",
    "automatic relation of information:\n",
    "what were the inputs when this model's gradient spiked?\n",
    "which models have been trained on this dataset,\n",
    "and what was their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below pull down the training data\n",
    "for the model currently running the FSDL Text Recognizer app.\n",
    "\n",
    "This is just intended as a demonstration of what's possible,\n",
    "so don't worry about understanding every piece of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from the same project that we used to look at the project view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_recognizer_project = wb_api.project(\"fsdl-text-recognizer-2021-training\", entity=\"cfrye59\")\n",
    "\n",
    "text_recognizer_project  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we search it for the text recognizer model currently being used in production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all versions of the text-recognizer ever put into production by...\n",
    "\n",
    "for art_type in text_recognizer_project.artifacts_types(): # looking through all artifact types\n",
    "    if art_type.name == \"prod-ready\":  # for the prod-ready type\n",
    "        # and grabbing the text-recognizer\n",
    "        production_text_recognizers = art_type.collection(\"paragraph-text-recognizer\").versions()\n",
    "\n",
    "# and then get the one that's currently being tested in CI by...\n",
    "for text_recognizer in production_text_recognizers:\n",
    "    if \"ci-test\" in text_recognizer.aliases:  # looking for the one that's labeled as CI-tested\n",
    "        in_prod_text_recognizer = text_recognizer\n",
    "\n",
    "# view its metadata at the url or in the notebook\n",
    "in_prod_text_recognizer_url = text_recognizer_project.url[:-9] + f\"artifacts/{in_prod_text_recognizer.type}/{in_prod_text_recognizer.name.replace(':', '/')}\"\n",
    "\n",
    "print(in_prod_text_recognizer_url)\n",
    "IFrame(src=in_prod_text_recognizer_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From its metadata, we can get information about how it was \"staged\" to be put into production,\n",
    "and in particular which model checkpoint was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_run = in_prod_text_recognizer.logged_by()\n",
    "\n",
    "training_ckpt, = [at for at in staging_run.used_artifacts() if at.type == \"model\"]\n",
    "training_ckpt.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That checkpoint was logged by a training experiment, which is available as metadata.\n",
    "\n",
    "We can look at the training run for that model, either here in the notebook or at its URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_run = training_ckpt.logged_by()\n",
    "print(training_run.url)\n",
    "IFrame(src=training_run.url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and from there, we can pull down the logged data and analyze it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_results = training_run.history(samples=10000)\n",
    "training_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = training_results.groupby(\"epoch\")[\"train/loss\"].mean().plot();\n",
    "training_results[\"validation/loss\"].dropna().plot(logy=True); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of our choices, like the depth of our network, the parameters of our optimizer,\n",
    "cannot be (easily) chosen by descent of the gradient of a loss function.\n",
    "\n",
    "These parameters that impact the values of the parameters we directly optimize with gradients,\n",
    "or _hyperparameters_\n",
    "can also be optimized.\n",
    "\n",
    "Simple and straightforward versions are best, and\n",
    "Weights & Biases makes the most straightforward forms of hyperparameter optimization easy.\n",
    "\n",
    "We can use the same training script and we don't need to run an optimization server.\n",
    "\n",
    "We just need to write a configuration yaml file. [docs](https://docs.wandb.ai/guides/sweeps/configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training/simple-sweep.yaml\n",
    "# first we specify what we're sweeping\n",
    "# we specify a program to run\n",
    "program: training/run_experiment.py\n",
    "# we optionally specify how to run it, including setting default arguments\n",
    "command:  \n",
    "    - ${env}\n",
    "    - ${interpreter}\n",
    "    - ${program}\n",
    "    - \"--wandb\"\n",
    "    - \"--overfit_batches\"\n",
    "    - \"1\"\n",
    "    - \"--log_every_n_steps\"\n",
    "    - \"25\"\n",
    "    - \"--max_epochs\"\n",
    "    - \"100\"\n",
    "    - \"--limit_test_batches\"\n",
    "    - \"0\"\n",
    "    - ${args}  # these arguments come from the sweep parameters below\n",
    "\n",
    "# and we specify which parameters to sweep over, what we're optimizing, and how we want to optimize it\n",
    "method: random  # generally, random searches perform well, can also be \"grid\" or \"bayes\"\n",
    "metric:\n",
    "    name: train/loss\n",
    "    goal: minimize\n",
    "parameters:  \n",
    "    # LineCNN hyperparameters\n",
    "    window_width:\n",
    "        values: [8, 16, 32, 64]\n",
    "    window_stride:\n",
    "        values: [4, 8, 16, 32]\n",
    "    # Transformer hyperparameters\n",
    "    tf_layers:\n",
    "        values: [1, 2, 4, 8]\n",
    "    # we can also fix some values, just like we set default arguments\n",
    "    gpus:\n",
    "        value: 1\n",
    "    model_class:\n",
    "        value: LineCNNTransformer\n",
    "    data_class:\n",
    "        value: IAMLines\n",
    "    loss:\n",
    "        value: transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the config we launch a \"controller\":\n",
    "a lightweight process that just decides what hyperparameters to try next\n",
    "and coordinates the heavierweight training.\n",
    "\n",
    "This lives on the W&B servers, so no headaches about opening communication,\n",
    "cleaning up when it's done, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb sweep training/simple-sweep.yaml --project fsdl-line-recognizer-2022\n",
    "simple_sweep_id = wb_api.project(\"fsdl-line-recognizer-2022\").sweeps()[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we can launch an \"agent\" to follow the orders of the controller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# interrupt twice to terminate this cell if it's running too long,\n",
    "#   can be over 15 minutes with some hyperparameters\n",
    "\n",
    "!wandb agent --project fsdl-line-recognizer-2022 --entity {wb_api.default_entity} --count=1 {simple_sweep_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the `--count` of runs to execute to just `1` to reduce the runtime.\n",
    "\n",
    "If not provided, the agent will run forever for random or Bayesian sweeps\n",
    "or until the sweep is terminated, which can be done from the W&B interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One fun trick: use environment variables to launch parallel epxeriments on multiple GPUs on the same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CUDA_VISIBLE_DEVICES=0 wandb agent $SWEEP_ID\n",
    "# open another terminal\n",
    "CUDA_VISIBLE_DEVICES=1 wandb agent $SWEEP_ID\n",
    "# and so on\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFx-OhF837Bp"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌟Contribute to a hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've kicked off a big hyperparameter search on the `LineCNNTransformer` that anyone can join!\n",
    "\n",
    "There are ~10,000,000 potential hyperparameter combinations,\n",
    "and each takes 30 minutes to test,\n",
    "so checking each possiblity will take over 500 years of compute time.\n",
    "Best get cracking then!\n",
    "\n",
    "Run the cell below to pull up a dashboard and print the URL where you can check on the current status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_entity = \"fullstackdeeplearning\"\n",
    "sweep_project = \"fsdl-line-recognizer-2022\"\n",
    "sweep_id = \"e0eo43eu\"\n",
    "sweep_url = f\"https://wandb.ai/{sweep_entity}/{sweep_project}/sweeps/{sweep_id}\"\n",
    "\n",
    "print(sweep_url)\n",
    "IFrame(src=sweep_url, width=\"100%\", height=frame_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also retrieve information about the sweep from the API,\n",
    "including the hyperparameters being swept over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_info = wb_api.sweep(\"/\".join([sweep_entity, sweep_project, sweep_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = sweep_info.config[\"parameters\"]\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to contribute to this sweep,\n",
    "run the cell below after changing the count to a number greater than 0.\n",
    "\n",
    "Each iteration runs for 30 minutes if it does not crash,\n",
    "e.g. due to out-of-memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0  # off by default, increase it to join in!\n",
    "\n",
    "if count:\n",
    "    !wandb agent {sweep_id} --entity {sweep_entity} --project {sweep_project} --count {count}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D39w0gXAiha"
   },
   "source": [
    "### 🌟🌟 Find good hyperparameters for the` LineCNNTransformer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you observe interesting phenomena during training,\n",
    "from promising hyperparameter combos to software bugs to strange model behavior,\n",
    "turn the charts into a report and share it with the FSDL community or\n",
    "[open an issue on GitHub](https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2022/issues)\n",
    "with a link to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the `sweep_info.config` above for hyperparameters or see the --help output for potential arguments\n",
    "%run training/run_experiment.py --model_class LineCNNTransformer --data_class IAMLines \\\n",
    "  --loss transformer --batch_size 32 --gpus {gpus} --max_epochs 5 \\\n",
    "  --log_every_n_steps 50 --wandb --limit_test_batches 0.1 \\\n",
    "  --limit_train_batches 0.1 --limit_val_batches 0.1 \\\n",
    "  --help\n",
    "    \n",
    "last_hyperparam_expt = wandb.run  # in case you wwant to look pull URLs, look up in API, etc., as in code above\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌟🌟🌟 Add logging of tensor statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchmetrics`. use `MinMetric`, `MaxMetric`, and `MeanMetric`. but start with just one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it with `training/run_experiment.py`, you'll need to\n",
    "- define it in a Python file somewhere, e.g. `text_recognizer/metrics`\n",
    "- add the metrics to `BaseImageToTextLitModel`'s `__init__` method, where `CharacterErrorRate` appears\n",
    "  - decide whether to calculate separate train/val versions (whatever you do, start with just one of them!)\n",
    "- in the appropriate methods of the `TransformerLitModel`, add metric calculation and logging for `Min`, `Max`, or `Mean`.\n",
    "  - base on the calculation and logging of `val_cer`\n",
    "  - `sync_dist=True` is only important in distributed settings, so you might not notice any issues regardless of that argument's value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus Challenge: use `MeanSquaredError` to implement a `VarianceMetric`. Hint: one way is to use `torch.zeros_like` and `torch.mean`."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMKpeodqRUzgu0VjkCVMBeJ",
   "collapsed_sections": [],
   "name": "lab04_experiments.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
