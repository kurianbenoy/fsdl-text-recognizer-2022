{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlH0lCOttCs5"
   },
   "source": [
    "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUPRHaeetRnT"
   },
   "source": [
    "# Lab 05: Troubleshooting & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bry3Hr-PcgDs"
   },
   "source": [
    "### What You Will Learn\n",
    "\n",
    "- Practices and tools for testing and linting Python code in general: `black`, `flake8`, `precommit`, `pytests` and `doctests`\n",
    "- How to implement memorization tests for ML training systems in particular\n",
    "- What a PyTorch training step looks like under the hood and how to troubleshoot performance bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs0LXXlCU6Ix"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkQiK7lkgeXm"
   },
   "source": [
    "If you're running this notebook on Google Colab,\n",
    "the cell below will run full environment setup.\n",
    "\n",
    "It should take about three minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVx7C7H0PIZC"
   },
   "outputs": [],
   "source": [
    "lab_idx = 5\n",
    "\n",
    "if \"bootstrap\" not in locals() or bootstrap.run:\n",
    "    # path management for Python\n",
    "    pythonpath, = !echo $PYTHONPATH\n",
    "    if \".\" not in pythonpath.split(\":\"):\n",
    "        pythonpath = \".:\" + pythonpath\n",
    "        %env PYTHONPATH={pythonpath}\n",
    "        !echo $PYTHONPATH\n",
    "\n",
    "    # get both Colab and local notebooks into the same state\n",
    "    !wget --quiet https://fsdl.me/gist-bootstrap -O bootstrap.py\n",
    "    import bootstrap\n",
    "\n",
    "    # change into the lab directory\n",
    "    bootstrap.change_to_lab_dir(lab_idx=lab_idx)\n",
    "\n",
    "    # allow \"hot-reloading\" of modules\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    # needed for inline plots in some contexts\n",
    "    %matplotlib inline\n",
    "\n",
    "    bootstrap.run = False  # change to True re-run setup\n",
    "    \n",
    "!pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sThWeTtV6fL_"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, IFrame\n",
    "\n",
    "full_width = True\n",
    "frame_height = 720  # adjust for your screen\n",
    "\n",
    "if full_width:  # if we want the notebook to take up the whole width\n",
    "    # add styling to the notebook's HTML directly\n",
    "    display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "    display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFP8lU4nSg1P"
   },
   "source": [
    "# Linting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to keep our code clean and uniform across developers.\n",
    "\n",
    "Applying the cleanliness checks and style rules should be\n",
    "as painless and automatic as possible.\n",
    "\n",
    "We recommend bundling tools together with [`pre-commit`](https://pre-commit.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pre-commit` separates the model development environment from the environments\n",
    "needed for the linting tools, preventing conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuQYOUy1X5R5"
   },
   "source": [
    "If you're working locally, might find it easier to use\n",
    "`make pip-tools-lint`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0XuIuKOXhJl"
   },
   "source": [
    "Run `pre-commit`. **MAKE SURE THIS WORKS IN LOCAL LAB DEV**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pre-commit run --all-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple hygiene things: accidental private key leaks, leftover debugger statements, merge conflicts, formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configured via yaml file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat .pre-commit-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a number of smaller cleanliness checks using hooks built by `pre-commit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat .pre-commit-config.yaml | grep repos -A 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the section of the file that applies most of our style enforcement with `flake8`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat .pre-commit-config.yaml | grep \"flake8 python\" -A 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_Q0BwQUXbg6"
   },
   "source": [
    "Show the `.flake8` file. Important to link these things, bidirectionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat .flake8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`select`: allow error codes that match these\n",
    "\n",
    "`extend-ignore`: bit not error codes that match these\n",
    "\n",
    "defines style out of all the things possibly checked.\n",
    "\n",
    "`per-file-ignores`: ignore specific warnings in specific files, an escape valve.\n",
    "\n",
    "for details on selecting and ignoring, see [`flake8` docs](https://flake8.pycqa.org/en/latest/user/violations.html)\n",
    "\n",
    "for definitions of the core error codes, see the [list in the docs](https://flake8.pycqa.org/en/latest/user/error-codes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these conventions come from [Python Enhancement Proposal 8](https://peps.python.org/pep-0008/),\n",
    "which exhorts you to \"know when to be inconsistent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder are configurations for the other `flake8` plugins that we use to define and enforce the rest of our style, with links to documentation:\n",
    "- [`flake8-import-order`](https://github.com/PyCQA/flake8-import-order) for checking imports\n",
    "- [`flake8-docstrings`](https://github.com/pycqa/flake8-docstrings) for docstring style\n",
    "- [`darglint`](https://github.com/terrencepreilly/darglint) for docstring completeness\n",
    "- [`flake8-annotations`](https://github.com/sco1/flake8-annotations) for type annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linting via a script and using `shellcheck`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYjpuFwjXkJc"
   },
   "source": [
    "To avoid needing to think about `pre-commit` while developing locally,\n",
    "we might put our linters into a shell script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat tasks/lint.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `shellcheck` on this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pre-commit run shellcheck --files tasks/lint.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encourage copying a script from their machine or a favorite repo.\n",
    "You'd be surprised at the classes of subtle bugs possible in bash!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Unofficial bash strict mode\" for louder failures in scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to reduce bugs is to use the \"unofficial bash strict mode\" settings, from the top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 3 tasks/lint.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core idea is to fail more loudly:\n",
    "`-u` means fail if a variable's value is `u`nset,\n",
    "ie not defined.\n",
    "Yes, bash allows you to reference undefined variables.\n",
    "Weird behavior of bash: it's just an empty string.\n",
    "\n",
    "`-o pipefail` means failures inside a pipe of commands propagate,\n",
    "rather than using the exit code of the last command.\n",
    "Kind of like `?` in typescript.\n",
    "Unix tools are happy to work on nonsense input,\n",
    "like sorting error messages or empty strings.\n",
    "\n",
    "Usually includes `-e`, here `+e` means exit codes do not cause an error.\n",
    "We want to run all of our linters, not just fail on the first one, so we do explicit error handling.\n",
    "\n",
    "Read more about these choices [here](http://redsymbol.net/articles/unofficial-bash-strict-mode/),\n",
    "including considerations for working with other non-conforming scripts\n",
    "and for resource-handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1XqsrU_XWWS"
   },
   "source": [
    "# Testing ML Codebases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPNzeq3NYF2W"
   },
   "source": [
    "## Testing Python code with `pytests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq5e_x6gc9Vu"
   },
   "source": [
    "\n",
    "ML codebases are Python first and foremost, so first let's get some Python tests going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sikt_SJXXZk9"
   },
   "source": [
    "Run `pytest`, look at output.\n",
    "\n",
    "[Docs](https://docs.pytest.org/en/7.1.x/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pytest ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end section there is a report of coverage from\n",
    "[`codecov`](https://about.codecov.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qOBHJnTZM9x"
   },
   "source": [
    "By default, `pytest` looks for files named `test_*.py` or `*_test.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls text_recognizer/tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's [good practice](https://docs.pytest.org/en/7.1.x/explanation/goodpractices.html#test-discovery)\n",
    "to separate these from the rest of your code, rather than scattering around the repo, in a folder or folders named `tests`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a specific example:\n",
    "the tests for some of our utilities around\n",
    "custom PyTorch Lightning `Callback`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.tests import test_callback_utils\n",
    "\n",
    "\n",
    "test_callback_utils.__doc__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can import this as a module! Keeping tests as simple as possible makes this easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is designed to prevent crashes:\n",
    "it checks for a particular type of error and turns it into a warning.\n",
    "\n",
    "Error-handling code is a common cause of bugs,\n",
    "so we test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_callback_utils.test_check_and_warn_simple??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic test, not incorporating any external libraries.\n",
    "\n",
    "This is the core functionality. Should be very non-flaky. Important for diagnosing a bug: which tests passed, not just which tests failed.\n",
    "\n",
    "This reasoning is explained in the docstrings, which are close to the code.\n",
    "Your test suite should be as welcoming as the rest of your codebase! The people reading it are likely upset\n",
    "and we want keep our time-to-resolve errors as short as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a specific error that triggered the addition of this code.\n",
    "\n",
    "So we test that it's handled as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_callback_utils.test_check_and_warn_tblogger??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That test can fail if the libraries around us change, ie if `TensorBoardLogger` gets a `log_table` method.\n",
    "\n",
    "But that will _also_ change the behavior of our code, and just because the method has the same name doesn't mean it does the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding error handling can also accidentally kill the happy path by raising an error incorrectly.\n",
    "\n",
    "So we explicitly test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_callback_utils.test_check_and_warn_wandblogger??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more tests we could build, e.g. manipulating classes and testing the behavior,\n",
    "testing more classes that might be targeted by `check_and_warn`,\n",
    "asserting that warnings are raised to the command line.\n",
    "\n",
    "But these three basic tests are likely to catch most changes that would break our code.\n",
    "\n",
    "If this utility starts to get more usage and become a critical path for lots of features, we can always add more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interleaving testing and documentation with `doctests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One function of tests is to build user/reader confidence in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function of documentation is to build user/reader knowledge in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are related. Let's put them together: code in docstring that gets tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.lit_models.util import first_appearance\n",
    "\n",
    "\n",
    "first_appearance??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can be used to e.g. quickly look for stop tokens,\n",
    "giving the length of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "first_appearance(torch.tensor([[1, 2, 3], [2, 3, 3], [1, 1, 1], [3, 1, 1]]), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the test by passing a command line argument to `pytest`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --doctest-modules text_recognizer/lit_models/util.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the\n",
    "[right configuration](https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2022/blob/627dc9dabc9070cb14bfe5bfcb1d6131eb7dc7a8/pyproject.toml#L12-L17),\n",
    "running `doctests` happens automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing our data, we mostly just use bare `assert`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"assert\" -r text_recognizer/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can organize some into a module for more clarity, incorporation into coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.tests.test_iam import test_iam_data_splits\n",
    "\n",
    "\n",
    "test_iam_data_splits??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also because this is in a module, we can easily run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iam_data_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we're checking something pretty simple here.\n",
    "\n",
    "What if we wanted to test more complex properties?\n",
    "We'll end up writing more complex code that might itself have subtle bugs,\n",
    "requiring tests for our tests and suffering from\n",
    "tester's regress\n",
    "([by analogy with experimenter's regress](https://en.wikipedia.org/wiki/Experimenter%27s_regress)):\n",
    "the validity of our tests is itself up for dispute requiring testing,\n",
    "which has disputable validity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a library or framework that is well-tested.\n",
    "\n",
    "More future-proof choice, with better features, is `great_expectations`.\n",
    "\n",
    "[Docs](https://docs.greatexpectations.io/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially with data, some tests are particularly \"heavy\" -- they take a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, testing whether the download of a dataset succeeds and gives the right checksum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytest` resolves this with `mark`s, which \"tag\" tests with names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pytest --markers | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose to run tests with a given mark with `-m`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -m \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to skip tests with a given mark, among other basic logical operations around combining and filtering tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -m \"not data and not slow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LuERxOXX_UJ"
   },
   "source": [
    "## Testing training with memorization tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also test training,\n",
    "the process by which data becomes models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training brings together data and models,\n",
    "so is dependent on both.\n",
    "\n",
    "So we decouple checking whether the script has a critical bug\n",
    "from whether the data or model code is broken\n",
    "by testing on some basic \"fake data\",\n",
    "based on a utility from `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls text_recognizer/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_recognizer.data import FakeImageData\n",
    "\n",
    "\n",
    "FakeImageData??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then test on the actual data with a smaller version of the real model.\n",
    "\n",
    "We use a smaller version so that these tests can run in just a few minutes on a CPU without acceleration.\n",
    "That means we can use tools like GitHub Actions.\n",
    "\n",
    "Here's the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat training/tests/test_run_experiment.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! ./training/tests/test_run_experiment.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script below runs a memorization test,\n",
    "checking whether our model can \"memorize\"\n",
    "the content of a single batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes up to two arguments:\n",
    "a `MAX`imum number of `EPOCHS` to run for and\n",
    "a `CRITERION` value of the loss to test against.\n",
    "\n",
    "The test passes if the loss is lower than the `CRITERION` value\n",
    "after the `MAX`imum number of `EPOCHS` has passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important line for this is the one that invokes our training script.\n",
    "\n",
    "This test has been tuned for maximum speed.\n",
    "Check each argument and understand why it's chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat training/tests/test_memorize_iam.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-47tUA_YNGe"
   },
   "source": [
    "Encourage them to try it out and look at results on W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_memorization = False\n",
    "\n",
    "if running_memorization:\n",
    "    max_epochs = 1000\n",
    "    loss_criterion = 0.1\n",
    "    !./training/tests/memorize-iam.sh {max_epochs} {loss_criterion}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgvlH433XvFf"
   },
   "source": [
    "# Automation with GitHub Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9VTPpteaA1K"
   },
   "source": [
    "Show the YAML file for automating `pre-commit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take a look at this locally,\n",
    "but note that there's a rich interface for looking at\n",
    "workflows, configurations, and executions on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_commit_action_url = \"https://github.com/full-stack-deep-learning/fsdl-text-recognizer-2022/actions/workflows/pre-commit.yml\"\n",
    "\n",
    "print(pre_commit_action_url)\n",
    "\n",
    "!cat .github/workflows/pre-commit.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow syntax docs [here](https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `name`:\n",
    "What is this called in the GitHub UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `on`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What triggers does this event have.\n",
    "\n",
    "`pull_request` and `push` are self-explanatory. Can also filter by branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`workflow_dispatch` allows manual triggering. Super important for debugging!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `jobs`:\n",
    "Each workflow can include multiple jobs, which may depend on each other, and so form a DAG.\n",
    "\n",
    "There's just one job here, `pre-commit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `steps`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside a job, there are \"steps\".\n",
    "\n",
    "Each step is a unit of work, they're executed in linear order.\n",
    "\n",
    "`uses` means we are applying an existing workflow as a step.\n",
    "\n",
    "We're just using existing workflows here. We don't need to write any commands ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4D_TODhb7CX"
   },
   "source": [
    "You can run this for yourself if you fork the lab repo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a more complex workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the test.yml file, but just the lines before the integration-tests start\n",
    "!wget -nv -qO- https://raw.githubusercontent.com/full-stack-deep-learning/fsdl-text-recognizer-2022/main/.github/workflows/test.yml  | grep \"integration-tests\" -B 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqjkqghmayn8"
   },
   "source": [
    "Simplest addition: environment variables, like `PYTHONPATH`.\n",
    "\n",
    "We also `run` our own commands, `pip` installing an environment and executing a test.\n",
    "\n",
    "Another level: secret variables. Inject eg API keys without revealing values. Configured via GitHub.\n",
    "\n",
    "Harder bits are for performance: multiple `jobs`, run independently. `actions/cache` caches environment to avoid needing to rebuild Python environment every time.\n",
    "\n",
    "protip: use a prefix like `v1` so you can manually trigger a cache miss if things are wonky. critical for debugging.\n",
    "\n",
    "see [blogpost from AIAI](https://blog.allenai.org/python-caching-in-github-actions-e9452698e98d)\n",
    "for more tips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPoFCoEcC8SV"
   },
   "source": [
    "# Troubleshooting model speed with the PyTorch Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting deep neural networks for speed is challenging.\n",
    "\n",
    "- Follow advice from others (Karpathy tweet, NVIDIA talk) and use existing implementations\n",
    "- Do empirical work, with good observations, to troubleshoot for speed\n",
    "- Truly understand distributed, accelerated tensor computations so you can write it correctly from scratch the first time\n",
    "\n",
    "For the full stack deep learning engineer,\n",
    "the last is typically out of reach,\n",
    "unless you're specializing in model performance.\n",
    "\n",
    "So we recommend reaching the second level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've added a new feature to `training/run_experiment.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python training/run_experiment.py --help | grep -A 1 -e \"^\\s*--profile\\s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this relies mostly on features of PyTorch Lightning,\n",
    "with just a few lines of customization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat training/run_experiment.py | grep args.profile -A 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on this, see the\n",
    "[Lightning tutorial](https://pytorch-lightning.readthedocs.io/en/1.6.1/advanced/profiler.html)\n",
    "on profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNVpGeQtQjMG"
   },
   "source": [
    "Heads up! Tools are a lot fiddlier here :/\n",
    "\n",
    "Also, the details depend on the precise machine being used -- GPU and CPU and RAM.\n",
    "\n",
    "If you don't observe the described phenomenon, check out the links to public pages with this information on W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4o3ylDgr46F",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from text_recognizer.data.base_data_module import DEFAULT_NUM_WORKERS\n",
    "\n",
    "\n",
    "# make it easier to separate these from training runs\n",
    "%env WANDB_JOB_TYPE=profile\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = DEFAULT_NUM_WORKERS\n",
    "gpus = 1  # must be run with accelerator\n",
    "\n",
    "%run training/run_experiment.py --wandb --profile \\\n",
    "  --max_epochs=1 \\\n",
    "  --num_sanity_val_steps=0 --limit_val_batches=0 --limit_test_batches=0 \\\n",
    "  --model_class=ResnetTransformer --data_class=IAMParagraphs --loss=transformer \\\n",
    "  --batch_size={batch_size} --num_workers={num_workers} --precision=16 --gpus=1\n",
    "\n",
    "latest_expt = wandb.run\n",
    "\n",
    "try:  # add execution trace to logged and versioned binaries\n",
    "    folder = wandb.run.dir\n",
    "    trace_matcher = wandb.run.dir + \"/*.pt.trace.json\"\n",
    "    trace_file = glob.glob(trace_matcher)[0]\n",
    "    trace_at = wandb.Artifact(name=f\"trace-{wandb.run.id}\", type=\"trace\")\n",
    "    trace_at.add_file(trace_file, name=\"training_step.pt.trace.json\")\n",
    "    wandb.log_artifact(trace_at)\n",
    "except IndexError:\n",
    "    print(\"trace not found\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePTkS3EqO5tN"
   },
   "source": [
    "We get out a table of statistics in the terminal,\n",
    "courtesy of Lightning.\n",
    "\n",
    "With practice, some useful information can be read out from this table,\n",
    "but it's better to start with both a less detailed and a more detailed view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzV62f3c7-Bi"
   },
   "source": [
    "## High-Level Statistics from the PyTorch Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNPKXkYw8NWd"
   },
   "source": [
    "Let's look at this info in a high-level TensorBoard dashboard, conveniently hosted for us on W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbItwuT88eAV"
   },
   "outputs": [],
   "source": [
    "your_tensorboard_url = latest_expt.url + \"/tensorboard\"\n",
    "\n",
    "print(your_tensorboard_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za2zybSwIo5C"
   },
   "outputs": [],
   "source": [
    "public_tensorboard_url = \"https://wandb.ai/cfrye59/fsdl-text-recognizer-2022/runs/z9ja0ngm/tensorboard\"\n",
    "print(public_tensorboard_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8wIjwrINE0B"
   },
   "source": [
    "### Overview Tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmBhUDgDLhd1"
   },
   "source": [
    "- Compute Capability: def'n from NVIDIA. Effectively, which features are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voUgT6zuLyi0"
   },
   "source": [
    "- GPU Utilization: fraction of time a kernel is running, anywhere on GPU. gross metric, first target. don't spend more on GPUs until you've got this to 80-90%+!\n",
    "- Est. SM Efficiency: **lookup precise def'n**\n",
    "- Est. Occupancy: **lookup precise def'n**. hard to get above 60%, requires very specialized techniques to get abvoe 80% **confirm by Google**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl-IndtXE4b4"
   },
   "source": [
    "- might see Tensor Cores -- from later generations **confirm details here, eg exact Compute Capability (7?)**, run much faster, require `precision=16`, so try cutting the batch size in half and running with `precision=32`. should get a performance recommendation to use half precision for Tensor Cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A88pQn4YMMKc"
   },
   "source": [
    "- Execution Summary: **check what it means to appear here**. does it mean we're waiting on this op? not really, bc kernel. is it \"kernel or something else?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mYzLLo4NTLu"
   },
   "source": [
    "### GPU Kernel Tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtlO15vvNVM5"
   },
   "source": [
    "- names of kernels being used.\n",
    "- hard to read, poorly documented, somewhat proprietary\n",
    "- but some useful bits, like `gemm` for matmuls, `winograd` and `conv` for convolutions\n",
    "- might look at to get a sense for where your model is spending time, but the better method is the trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQwrPY_H77H8"
   },
   "source": [
    "## Going deeper with the Chrome Trace Viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web developers have amazing tools.\n",
    "\n",
    "We steal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_UNhzrr5xQz"
   },
   "outputs": [],
   "source": [
    "trace_files_url = latest_expt.url.split(\"/runs/\")[0] + f\"/artifacts/trace/trace-{latest_expt.id}/latest/files/\"\n",
    "trace_url = trace_files_url + \"training_step.pt.trace.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w9F2UA7Qctg"
   },
   "source": [
    "Can be flaky. Open page -> in W&B if so.\n",
    "\n",
    "If you're having trouble finding the features referred to below,\n",
    "it could be due to hardware differences.\n",
    "In that case, read the below while looking at\n",
    "[this example trace](https://wandb.ai/cfrye59/fsdl-text-recognizer-2022-training/artifacts/trace/trace-67j1qxws/latest/files/training_step.pt.trace.json)\n",
    "and then return to examine your trace once you understand the trace viewer better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMUs4aby6Rfd"
   },
   "outputs": [],
   "source": [
    "print(trace_url)\n",
    "IFrame(src=trace_url, height=frame_height * 1.5, width=\"100%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXMcBhnCgdN_"
   },
   "source": [
    "microsecond-level detail on what's going on inside `training_step`.\n",
    "\n",
    "\"call stack\" -- methods at the top call the method beneath them.\n",
    "\n",
    "`training_step` is towards the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBEFgtRCKqrh"
   },
   "source": [
    "Let's orient ourselves with some gross features.\n",
    "\n",
    "### The forwards pass\n",
    "\n",
    "Type in `resnet` to the search bar in the top-right.\n",
    "\n",
    "This will highlight the first part of our forwards pass.\n",
    "\n",
    "Should say `thread XYZ (python)` next to it.\n",
    "\n",
    "Then type in `transformer` to highlight the second part.\n",
    "Should be at same height in same section.\n",
    "\n",
    "Zoom in (arrows in the floating toolbar) to view in detail.\n",
    "Clear the search bar so that the trace is in color.\n",
    "\n",
    "Start at a very abstract level in Python (\"`Sequential`\", `Conv2d`)\n",
    "end up with very precise `cudnn` and `cuda` operations\n",
    "(`aten::cudnn_convolution`).\n",
    "\n",
    "`aten` ([no relation to the Pharaoh](https://twitter.com/charles_irl/status/1422232585724432392?s=20&t=Jr4j5ZXhV20xGwUVD1rY0Q))\n",
    "is the PyTorch tensor math library\n",
    "that abstracts over specific backends like `cudnn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBEFgtRCKqrh"
   },
   "source": [
    "### GPU kernel execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Towards the bottom, should see a section labeled \"GPU\".\n",
    "\n",
    "Within it, you'll see one or more \"`stream`s\".\n",
    "These are the units of work on a GPU,\n",
    "akin to threads on the CPU.\n",
    "\n",
    "When there are colored bars in this area,\n",
    "the GPU is doing work of some kind.\n",
    "The fraction of this bar that is filled in with color\n",
    "is the same as the \"GPU Utilization %\" we've seen previously.\n",
    "\n",
    "In CUDA, work is queued up to be placed into streams and completed\n",
    "in a distributed and asynchronous manner.\n",
    "\n",
    "The selection of work is happening on the CPU,\n",
    "as we saw above.\n",
    "The CPU and the GPU work together to coordinate this process.\n",
    "\n",
    "Type `cuda` into the search bar and you'll see these coordination operations happening:\n",
    "`cudaLaunchKernel`, for example, is the CPU telling the GPU what to do.\n",
    "\n",
    "Running the same PyTorch model in different versions of PyTorch,\n",
    "on different GPUs, and even on tensors of different sizes will result\n",
    "in different choices of concrete kernel operation,\n",
    "e.g. different matrix multiplication algorithms.\n",
    "\n",
    "Type `sync` into the search bar and you'll see places where either work on the GPU\n",
    "or work on the CPU needs to await synchronization,\n",
    "e.g. copying data from the CPU to the GPU or deciding what to do next\n",
    "on the basis of the contents of a tensor.\n",
    "\n",
    "If you see a \"sync\" block above an area where the stream on the GPU is empty,\n",
    "you've got a performance bottleneck.\n",
    "That's a good place to review your code to understand why the synchronization is happening\n",
    "and removing it if it's not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBEFgtRCKqrh"
   },
   "source": [
    "### The backwards pass\n",
    "\n",
    "Type in `backward` to the search bar.\n",
    "\n",
    "This will highlight mostly components of our backwards pass.\n",
    "\n",
    "Generally, this happens in a separate thread from the forwards pass,\n",
    "also on the CPU.\n",
    "\n",
    "Similarly launches kernels on GPU from CPU.\n",
    "\n",
    "Generally, there's no need to optimize the backwards pass --\n",
    "removing bottlenecks in the forwards pass results in a fast backwards pass.\n",
    "\n",
    "One reason why is that these two passes are the transpose of another,\n",
    "so they share a lot of properties,\n",
    "and bottlenecks in one become bottlenecks in the other.\n",
    "But the forwards pass is under our control,\n",
    "so it's easier to reason about.\n",
    "\n",
    "Another reason is that the forwards pass is harder because PyTorch doesn't know what's happening next.\n",
    "Backwards passes, on the other hand, are happening once with a static compute graph,\n",
    "so more optimizations are possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBEFgtRCKqrh"
   },
   "source": [
    "### The optimizer step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type in `Adam.step` to the search bar to highlight the actions of the optimizer.\n",
    "\n",
    "Lightning implementation detail:\n",
    "`optimizer_step` actually wraps the forwards and backwards pass,\n",
    "because some optimizers require multiple calls to forward/backward.\n",
    "So reading the stats makes it look like the `optimizer` takes up all the time,\n",
    "even though the actual calculations and updates by the optimizer are not taking that much time.\n",
    "\n",
    "One immediately obvious bit:\n",
    "our GPU utilization is not great.\n",
    "\n",
    "We're looping over parameters,\n",
    "in Python,\n",
    "and applying the ADAM update rules to each,\n",
    "resulting in the launch of a number of kernels\n",
    "proportional to the number of layers in the model.\n",
    "\n",
    "As of writing in August 2022,\n",
    "more efficient optimizers are not a stable part of PyTorch,\n",
    "which is at v1.12, but\n",
    "[there is an unstable API](https://github.com/pytorch/pytorch/issues/68041)\n",
    "and stable implementations outside of PyTorch, e.g.\n",
    "[in NVIDIA's `apex` library](https://nvidia.github.io/apex/optimizers.html),\n",
    "not to be confused with the\n",
    "[Apex Optimizers Project](https://www.apexoptimizers.com/),\n",
    "which is a collection of fitness-themed cheetah NFTs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-aways for PyTorch performance bottleneck troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here was to learn some basic principles and tools for bottlenecking\n",
    "the most common issues and the lowest-hanging fruit in PyTorch code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwHwJkVMHYGA"
   },
   "source": [
    "Towards that goal, we viewed the trace to get an understanding of\n",
    "what's going on inside a PyTorch training step,\n",
    "in terms of a \"host\", generally the CPU, and a \"device\", here the GPU.\n",
    "\n",
    "- host moves through compute graph, not caring about content unless it has to. it's just teeing up.\n",
    "- the host has metadata, like type and shape, which it uses to select operations. convolutions with very large filter sizes, for example, might use fast Fourier transform-based convolution algorithms, while the smaller filter sizes typical of contemporary CNNs.\n",
    "- device executes actual operations.\n",
    "\n",
    "And how to optimize it:\n",
    "- goal: Python can slowly chew its way through looking up the right CUDA kernel and telling the GPU that's what it needs in time before the previous kernel finishes.\n",
    "- Ideally, we're actually getting far ahead of execution. The CPU makes it all the way through the backwards pass before the GPU is done.\n",
    "- It's more like a navigator who is steering a ship. They move their finger over the route on the map, delivering steering directions, at the same time as the ship moves over the route in the real world. The navigator needs to know where they are going before they can give steering directions, and so long as they can just use the map and never have to \"go to visual\" and look at the environment, the ship will never be sitting idle waiting on the navigator.\n",
    "- operationalization: 100% GPU utilization, meaning a kernel is running at all times. this is the aggregate metric reported in the systems tab on W&B or in the output of `!nvidia-smi`\n",
    "- hardmode: high occupancy **(check definition)**. getting to 80%+ is challenging.\n",
    "- sharp edge: some operations require knowledge not available until the value is computed, see the `type_as` operation, which causes a synchronization between host and device\n",
    "- sharp edge: Python is very slow, so if you throw in a really slow Python operation, like dynamically creating classes or iterating over a bunch of bytes, esp from disk, that can easily dwarf the \"actually hard\" part running in a fast language (C++) on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHS5MpMZRzOE"
   },
   "source": [
    "Common issue: the `DataLoader`s are the bottleneck.\n",
    "\n",
    "See suggestions from `timm` author Wightman. FFCV library. Hugging Face `FastTokenizers` with Rust acceleration.\n",
    "\n",
    "Faster CPUs can be important, and good network connections!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0wW2_lRKfY1"
   },
   "source": [
    "But high utilization does not mean high efficiency. Just spinning the GPUs is enough there.\n",
    "\n",
    "For example, double precision floats. Fun example: results in convolutions taking longer than anything else, since 64 bit floats don't have as fast of conv implementations.\n",
    "\n",
    "Synchronization events between GPUs.\n",
    "\n",
    "Always need to also consider examples per second -- and the gold star is _decrease in loss per second_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFzPsplfdo_o"
   },
   "source": [
    "For PyTorch internals abstractly, see Ed Yang's blogpost.\n",
    "\n",
    "For performance considerations in PyTorch, see Horace Xu's blogpost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFx-OhF837Bp"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq6-S6TC38AY"
   },
   "source": [
    "### 🌟 Compare `num_workers=0` and `num_workers=1`\n",
    "\n",
    "For a comparison between `0` and `DEFAULT_NUM_WORKERS`, see results [here](https://wandb.ai/cfrye59/fsdl-text-recognizer-2022-training/artifacts/trace/trace-2eddoiz7/v0/files/training_step.pt.trace.json#f388e363f107e21852d5$trace-67j1qxws).\n",
    "\n",
    "Our dataset here fits in RAM, so large numbers of workers don't tend to cause issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5D39w0gXAiha"
   },
   "source": [
    "### 🌟🌟 Resolve issues with a file by fixing flake8 lints, then write a test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSon2fB5VVM_"
   },
   "source": [
    "Add links for `flake8` error codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOnGUqy8Vieq"
   },
   "source": [
    "Add a simple function here -- some basic torch operations with a clear name?\n",
    "\n",
    "Write bad docs that trigger flakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYiRvU4HA84t"
   },
   "outputs": [],
   "source": [
    "%%writefile training/fixme.py\n",
    "import torch\n",
    "from training import run_experiment\n",
    "from numpy import *\n",
    "import random\n",
    "from pathlib import Path\n",
    "def foo(model):\n",
    "  return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXJpmvuzT1w0"
   },
   "outputs": [],
   "source": [
    "!pre-commit run black --files training/fixme.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRO-oJfdUrcQ"
   },
   "outputs": [],
   "source": [
    "!cat training/fixme.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jM8NHxVbSEQD"
   },
   "outputs": [],
   "source": [
    "!pre-commit run --files training/fixme.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab05_troubleshooting.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
